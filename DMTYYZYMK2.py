import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import pandas as pd
from PIL import Image, ImageEnhance, ImageFilter
import torchvision.transforms as transforms
from transformers import (
    BlipProcessor,
    BlipForConditionalGeneration,
    get_linear_schedule_with_warmup
)
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import numpy as np
import warnings

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­ç¡®ä¿å¯é‡å¤æ€§
torch.manual_seed(42)
np.random.seed(42)


def prepare_data(data_path):
    """å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®"""
    defect_types = ['Cr', 'In', 'Pa', 'PS', 'RS', 'Sc']
    image_paths = []
    labels = []

    for defect in defect_types:
        defect_path = os.path.join(data_path, defect)

        if os.path.exists(defect_path):
            for img_file in os.listdir(defect_path):
                if img_file.lower().endswith(('.bmp', '.png', '.jpg', '.jpeg')):
                    img_path = os.path.join(defect_path, img_file)
                    image_paths.append(img_path)
                    labels.append(defect)

    if len(image_paths) == 0:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶ï¼")
        return [], [], [], [], [], []

    print(f"æ‰¾åˆ°æ€»è®¡ {len(image_paths)} å¼ å›¾åƒ")

    # åˆ’åˆ†æ•°æ®é›†
    train_paths, test_paths, train_labels, test_labels = train_test_split(
        image_paths, labels, test_size=0.2, random_state=42, stratify=labels
    )

    train_paths, val_paths, train_labels, val_labels = train_test_split(
        train_paths, train_labels, test_size=0.15, random_state=42, stratify=train_labels
    )

    return train_paths, val_paths, test_paths, train_labels, val_labels, test_labels


class RobustSurfaceDefectDataset(Dataset):
    """å¢å¼ºçš„å·¥ä¸šç¼ºé™·æ•°æ®é›†ç±»ï¼Œæ”¯æŒå¤æ‚ç¯å¢ƒæ¡ä»¶"""

    def __init__(self, image_paths, labels, transform=None, is_train=True,
                 confidence_threshold=0.8, augment_intensity=1.0):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        self.is_train = is_train
        self.confidence_threshold = confidence_threshold
        self.augment_intensity = augment_intensity

        # è¯¦ç»†çš„ç¼ºé™·æè¿°åº“
        self.defect_descriptions = {
            'Cr': {
                'description': 'è£‚çº¹ç¼ºé™·ï¼Œè¡¨ç°ä¸ºçº¿æ€§æ–­è£‚ç‰¹å¾',
                'severity': 'é«˜å±',
                'location': 'è¡¨é¢æˆ–å†…éƒ¨ç»“æ„',
                'suggestion': 'ç«‹å³åœæ­¢ä½¿ç”¨ï¼Œè¿›è¡Œç»“æ„æ€§è¯„ä¼°ï¼Œè€ƒè™‘æ›´æ¢é›¶ä»¶'
            },
            'In': {
                'description': 'å¤¹æ‚ç¼ºé™·ï¼Œææ–™ä¸­å«æœ‰å¼‚ç‰©æ‚è´¨',
                'severity': 'ä¸­å±',
                'location': 'ææ–™å†…éƒ¨',
                'suggestion': 'è¯„ä¼°æ‚è´¨å½±å“èŒƒå›´ï¼Œè¿›è¡Œæ— æŸæ£€æµ‹ç¡®å®šå†…éƒ¨æƒ…å†µ'
            },
            'Pa': {
                'description': 'æ–‘å—ç¼ºé™·ï¼Œè¡¨é¢å‡ºç°ä¸å‡åŒ€è‰²æ–‘æˆ–è…èš€',
                'severity': 'ä½å±',
                'location': 'è¡¨é¢åŒºåŸŸ',
                'suggestion': 'è¿›è¡Œè¡¨é¢æ¸…ç†å’Œé˜²è…å¤„ç†ï¼Œå®šæœŸç›‘æ§æ–‘å—å˜åŒ–'
            },
            'PS': {
                'description': 'ç‚¹èš€ç¼ºé™·ï¼Œè¡¨é¢å‡ºç°å±€éƒ¨ç‚¹çŠ¶è…èš€',
                'severity': 'ä¸­å±',
                'location': 'è¡¨é¢å±€éƒ¨åŒºåŸŸ',
                'suggestion': 'è¯„ä¼°ç‚¹èš€æ·±åº¦ï¼Œè¿›è¡Œè¡¨é¢ä¿®å¤å’Œé˜²è…æ¶‚å±‚å¤„ç†'
            },
            'RS': {
                'description': 'éº»é¢ç¼ºé™·ï¼Œè¡¨é¢ç²—ç³™ä¸å¹³æ•´',
                'severity': 'ä½å±',
                'location': 'æ•´ä¸ªè¡¨é¢',
                'suggestion': 'è¿›è¡Œè¡¨é¢æ‰“ç£¨å¤„ç†ï¼Œæ”¹å–„è¡¨é¢å…‰æ´åº¦'
            },
            'Sc': {
                'description': 'åˆ’ç—•ç¼ºé™·ï¼Œè¡¨é¢æœ‰æ˜æ˜¾çš„çº¿æ€§åˆ®ç—•',
                'severity': 'ä½å±',
                'location': 'è¡¨é¢ç‰¹å®šæ–¹å‘',
                'suggestion': 'è¯„ä¼°åˆ’ç—•æ·±åº¦ï¼Œè¿›è¡Œè¡¨é¢ä¿®å¤ï¼ŒåŠ å¼ºæ“ä½œè§„èŒƒ'
            }
        }

    def __len__(self):
        return len(self.image_paths)

    def apply_industrial_augmentation(self, image):
        """å·¥ä¸šç¯å¢ƒå¢å¼ºï¼šæ¨¡æ‹Ÿå…‰çº¿å˜åŒ–ã€è§’åº¦å˜åŒ–ç­‰å®é™…æ¡ä»¶"""
        if not self.is_train or self.augment_intensity == 0:
            return image

        # å…‰çº¿å˜åŒ–å¢å¼º
        if np.random.random() < 0.6 * self.augment_intensity:
            brightness_factor = np.random.uniform(0.7, 1.3)
            contrast_factor = np.random.uniform(0.8, 1.2)
            image = ImageEnhance.Brightness(image).enhance(brightness_factor)
            image = ImageEnhance.Contrast(image).enhance(contrast_factor)

        # æ¨¡æ‹Ÿè§’åº¦å˜åŒ–
        if np.random.random() < 0.5 * self.augment_intensity:
            angle = np.random.uniform(-15, 15)
            image = image.rotate(angle, resample=Image.BILINEAR, expand=False)

        # æ¨¡æ‹Ÿå™ªå£°å’Œæ¨¡ç³Š
        if np.random.random() < 0.3 * self.augment_intensity:
            image = image.filter(ImageFilter.GaussianBlur(radius=0.5))

        return image

    def __getitem__(self, idx):
        try:
            img_path = self.image_paths[idx]
            label = self.labels[idx]

            image = Image.open(img_path).convert('RGB')

            # åº”ç”¨å·¥ä¸šç¯å¢ƒå¢å¼º
            image = self.apply_industrial_augmentation(image)

            if self.transform:
                image = self.transform(image)

            defect_type = label
            defect_info = self.defect_descriptions.get(defect_type, {
                'description': 'æœªçŸ¥ç¼ºé™·ç±»å‹',
                'severity': 'å¾…è¯„ä¼°',
                'location': 'å¾…ç¡®è®¤',
                'suggestion': 'è¯·è¿›è¡Œè¿›ä¸€æ­¥æ£€æµ‹åˆ†æ'
            })

            # ç”Ÿæˆæ›´ä¸“ä¸šçš„æŠ¥å‘Šæ–‡æœ¬
            report_text = (
                f"æ£€æµ‹åˆ°{defect_type}ç±»å‹ç¼ºé™·ã€‚{defect_info['description']}ã€‚"
                f"ç¼ºé™·ä½ç½®ï¼š{defect_info['location']}ï¼Œä¸¥é‡ç¨‹åº¦ï¼š{defect_info['severity']}ã€‚"
                f"å¤„ç†å»ºè®®ï¼š{defect_info['suggestion']}ã€‚"
            )

            return {
                'image': image,
                'label': label,
                'report_text': report_text,
                'defect_type': defect_type,
                'defect_info': defect_info
            }

        except Exception as e:
            print(f"Error loading image {img_path}: {str(e)}")
            return None


def create_industrial_transforms():
    """åˆ›å»ºå·¥ä¸šçº§å›¾åƒé¢„å¤„ç†å˜æ¢ï¼Œå¢å¼ºé²æ£’æ€§"""

    # è®­ç»ƒé˜¶æ®µï¼šæ›´å¼ºçš„æ•°æ®å¢å¼º
    train_transform = transforms.Compose([
        transforms.Resize((384, 384)),
        transforms.RandomHorizontalFlip(p=0.4),
        transforms.RandomRotation(degrees=10),
        transforms.ColorJitter(
            brightness=0.3,
            contrast=0.3,
            saturation=0.2,
            hue=0.1
        ),
        transforms.RandomAffine(
            degrees=0,
            translate=(0.1, 0.1),
            scale=(0.9, 1.1)
        ),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    # æ¨ç†é˜¶æ®µ
    eval_transform = transforms.Compose([
        transforms.Resize((384, 384)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
    ])

    return train_transform, eval_transform


class ProductionReadyDefectModel(nn.Module):
    """ç”Ÿäº§ç¯å¢ƒå°±ç»ªçš„ç¼ºé™·æ£€æµ‹æ¨¡å‹ï¼Œä¿®å¤BatchNormé—®é¢˜"""

    def __init__(self, num_defect_classes=6, text_model_name='./models/blip-image-captioning-base',
                 use_quantization=False):
        super(ProductionReadyDefectModel, self).__init__()

        # åŠ è½½é¢„è®­ç»ƒBLIPæ¨¡å‹
        self.blip_model = BlipForConditionalGeneration.from_pretrained(text_model_name)

        # é‡åŒ–æ”¯æŒ
        if use_quantization:
            self.blip_model = torch.quantization.quantize_dynamic(
                self.blip_model, {nn.Linear}, dtype=torch.qint8
            )

        vision_config = self.blip_model.config.vision_config
        self.vision_hidden_size = vision_config.hidden_size

        # ä¿®å¤çš„ç¼ºé™·åˆ†ç±»å¤´ï¼šç§»é™¤BatchNormï¼Œä½¿ç”¨æ›´ç¨³å®šçš„ç»“æ„
        self.defect_classifier = nn.Sequential(
            nn.Linear(self.vision_hidden_size, 512),
            # ç§»é™¤äº†BatchNorm1dï¼Œå› ä¸ºå®ƒéœ€è¦æ‰¹æ¬¡å¤§å°>1
            nn.ReLU(inplace=True),
            nn.Dropout(0.4),
            nn.Linear(512, 256),
            nn.ReLU(inplace=True),
            nn.Dropout(0.3),
            nn.Linear(256, num_defect_classes)
        )

        self.classification_criterion = nn.CrossEntropyLoss()

    def get_vision_features(self, pixel_values):
        """å¢å¼ºçš„ç‰¹å¾æå–"""
        vision_outputs = self.blip_model.vision_model(
            pixel_values=pixel_values,
            return_dict=True
        )

        last_hidden_state = vision_outputs.last_hidden_state
        vision_features = last_hidden_state[:, 0, :]  # å–ç¬¬ä¸€ä¸ªtokenä½œä¸ºå›¾åƒç‰¹å¾

        return vision_features

    def forward(self, pixel_values, input_ids=None, attention_mask=None, labels=None, defect_labels=None):
        vision_features = self.get_vision_features(pixel_values)
        defect_logits = self.defect_classifier(vision_features)

        text_outputs = None
        text_loss = None

        if input_ids is not None:
            text_outputs = self.blip_model(
                pixel_values=pixel_values,
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels,
                return_dict=True
            )
            text_loss = text_outputs.loss

        classification_loss = None
        if defect_labels is not None:
            classification_loss = self.classification_criterion(defect_logits, defect_labels)

        return {
            'text_loss': text_loss,
            'classification_loss': classification_loss,
            'defect_logits': defect_logits,
            'logits': text_outputs.logits if text_outputs is not None else None,
            'vision_features': vision_features
        }


def train_model_with_monitoring(model, train_loader, val_loader, processor, device, num_epochs=5):
    """å¸¦ç›‘æ§çš„è®­ç»ƒå‡½æ•°ï¼Œä¿®å¤æ‰¹æ¬¡å¤§å°é—®é¢˜"""

    optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)
    total_steps = len(train_loader) * num_epochs

    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(0.1 * total_steps),
        num_training_steps=total_steps
    )

    # è®­ç»ƒç›‘æ§å˜é‡
    best_accuracy = 0.0
    patience = 3
    patience_counter = 0
    training_history = []

    for epoch in range(num_epochs):
        model.train()
        train_loss = 0.0
        train_correct = 0
        train_total = 0

        for batch_idx, batch in enumerate(train_loader):
            if batch is None:
                continue

            # è¿‡æ»¤æ‰Noneæ ·æœ¬ï¼Œç¡®ä¿æ‰¹æ¬¡å®Œæ•´æ€§
            valid_samples = []
            for i in range(len(batch['image'])):
                if batch['image'][i] is not None:
                    valid_samples.append(i)

            if len(valid_samples) < 2:  # ç¡®ä¿æ‰¹æ¬¡å¤§å°è‡³å°‘ä¸º2
                continue

            # åªä½¿ç”¨æœ‰æ•ˆæ ·æœ¬
            images = torch.stack([batch['image'][i] for i in valid_samples]).to(device)
            report_texts = [batch['report_text'][i] for i in valid_samples]

            defect_labels_indices = []
            for i in valid_samples:
                label = batch['defect_type'][i]
                try:
                    idx = ['Cr', 'In', 'Pa', 'PS', 'RS', 'Sc'].index(label)
                    defect_labels_indices.append(idx)
                except ValueError:
                    continue

            if len(defect_labels_indices) < 2:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ ‡ç­¾
                continue

            defect_labels = torch.tensor(defect_labels_indices).to(device)

            text_inputs = processor(
                text=report_texts,
                padding='longest',
                return_tensors='pt',
                max_length=128,
                truncation=True
            )

            optimizer.zero_grad()

            outputs = model(
                pixel_values=images,
                input_ids=text_inputs['input_ids'].to(device),
                attention_mask=text_inputs['attention_mask'].to(device),
                labels=text_inputs['input_ids'].to(device),
                defect_labels=defect_labels
            )

            alpha = 0.7
            if outputs['text_loss'] is not None and outputs['classification_loss'] is not None:
                total_loss = alpha * outputs['text_loss'] + (1 - alpha) * outputs['classification_loss']
            elif outputs['text_loss'] is not None:
                total_loss = outputs['text_loss']
            elif outputs['classification_loss'] is not None:
                total_loss = outputs['classification_loss']
            else:
                continue

            total_loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
            optimizer.step()
            scheduler.step()

            train_loss += total_loss.item()

            # è®¡ç®—è®­ç»ƒå‡†ç¡®ç‡
            _, predicted = torch.max(outputs['defect_logits'], 1)
            train_correct += (predicted == defect_labels).sum().item()
            train_total += defect_labels.size(0)

            if batch_idx % 10 == 0:
                text_loss_val = outputs['text_loss'].item() if outputs['text_loss'] is not None else 0
                class_loss_val = outputs['classification_loss'].item() if outputs[
                                                                              'classification_loss'] is not None else 0
                batch_accuracy = (predicted == defect_labels).float().mean().item()

                print(f'Epoch {epoch + 1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, '
                      f'Total Loss: {total_loss.item():.4f}, '
                      f'Text Loss: {text_loss_val:.4f}, '
                      f'Class Loss: {class_loss_val:.4f}, '
                      f'Batch Acc: {batch_accuracy:.4f}')

        # è¯¦ç»†éªŒè¯è¯„ä¼°
        model.eval()
        val_loss = 0.0
        all_predictions = []
        all_labels = []

        with torch.no_grad():
            for batch in val_loader:
                if batch is None:
                    continue

                # åŒæ ·è¿‡æ»¤éªŒè¯é›†çš„æ— æ•ˆæ ·æœ¬
                valid_samples = []
                for i in range(len(batch['image'])):
                    if batch['image'][i] is not None:
                        valid_samples.append(i)

                if len(valid_samples) < 2:
                    continue

                images = torch.stack([batch['image'][i] for i in valid_samples]).to(device)
                report_texts = [batch['report_text'][i] for i in valid_samples]

                defect_labels_indices = []
                for i in valid_samples:
                    label = batch['defect_type'][i]
                    try:
                        idx = ['Cr', 'In', 'Pa', 'PS', 'RS', 'Sc'].index(label)
                        defect_labels_indices.append(idx)
                    except ValueError:
                        continue

                if len(defect_labels_indices) < 2:
                    continue

                defect_labels = torch.tensor(defect_labels_indices).to(device)

                text_inputs = processor(
                    text=report_texts,
                    padding='longest',
                    return_tensors='pt',
                    max_length=128,
                    truncation=True
                )

                outputs = model(
                    pixel_values=images,
                    input_ids=text_inputs['input_ids'].to(device),
                    attention_mask=text_inputs['attention_mask'].to(device),
                    labels=text_inputs['input_ids'].to(device),
                    defect_labels=defect_labels
                )

                if outputs['text_loss'] is not None and outputs['classification_loss'] is not None:
                    total_loss = 0.7 * outputs['text_loss'] + 0.3 * outputs['classification_loss']
                elif outputs['text_loss'] is not None:
                    total_loss = outputs['text_loss']
                elif outputs['classification_loss'] is not None:
                    total_loss = outputs['classification_loss']
                else:
                    continue

                val_loss += total_loss.item()

                _, predicted = torch.max(outputs['defect_logits'], 1)
                all_predictions.extend(predicted.cpu().numpy())
                all_labels.extend(defect_labels.cpu().numpy())

        # è®¡ç®—è¯¦ç»†æŒ‡æ ‡
        if len(all_predictions) > 0:
            val_accuracy = np.sum(np.array(all_predictions) == np.array(all_labels)) / len(all_predictions)
        else:
            val_accuracy = 0.0

        avg_train_loss = train_loss / len(train_loader) if len(train_loader) > 0 else 0
        avg_val_loss = val_loss / len(val_loader) if len(val_loader) > 0 else 0

        # æ—©åœæœºåˆ¶
        if val_accuracy > best_accuracy:
            best_accuracy = val_accuracy
            patience_counter = 0
            # ä¿å­˜æœ€ä½³æ¨¡å‹
            torch.save(model.state_dict(), 'best_defect_detection_model.pth')
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"Early stopping at epoch {epoch + 1}")
            break

        print(f'Epoch {epoch + 1}/{num_epochs}:')
        print(f'  Train Loss: {avg_train_loss:.4f}, Train Acc: {train_correct / train_total:.4f}')
        print(f'  Val Loss: {avg_val_loss:.4f}, Val Acc: {val_accuracy:.4f}')
        print(f'  Best Accuracy: {best_accuracy:.4f}')
        print('-' * 50)

        training_history.append({
            'epoch': epoch + 1,
            'train_loss': avg_train_loss,
            'val_loss': avg_val_loss,
            'val_accuracy': val_accuracy
        })

    return training_history


def generate_production_report(model, image_path, processor, device, confidence_threshold=0.8):
    """ç”Ÿæˆç”Ÿäº§çº§è´¨æ£€æŠ¥å‘Š"""

    eval_transform = transforms.Compose([
        transforms.Resize((384, 384)),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    image = Image.open(image_path).convert('RGB')
    image_tensor = eval_transform(image).unsqueeze(0).to(device)

    model.eval()

    with torch.no_grad():
        # ç”Ÿæˆæ–‡æœ¬æè¿°
        generated_ids = model.blip_model.generate(
            pixel_values=image_tensor,
            max_length=100,
            num_beams=3,
            early_stopping=True
        )

        generated_text = processor.decode(generated_ids[0], skip_special_tokens=True)

        # ç¼ºé™·åˆ†ç±»
        vision_features = model.get_vision_features(image_tensor)
        defect_logits = model.defect_classifier(vision_features)
        predicted_class = torch.argmax(defect_logits, dim=1).item()
        confidence = torch.softmax(defect_logits, dim=1)[0][predicted_class].item()

        defect_types = ['Cr', 'In', 'Pa', 'PS', 'RS', 'Sc']
        predicted_defect = defect_types[predicted_class]

        # ç½®ä¿¡åº¦æ§åˆ¶æœºåˆ¶
        needs_manual_review = confidence < confidence_threshold
        review_note = "ã€éœ€äººå·¥å¤æ ¸ã€‘" if needs_manual_review else "ã€è‡ªåŠ¨æ£€æµ‹é€šè¿‡ã€‘"

        # æ„å»ºç”Ÿäº§çº§æŠ¥å‘Š
        defect_descriptions = {
            'Cr': {'severity': 'é«˜å±', 'suggestion': 'ç«‹å³åœæ­¢ä½¿ç”¨ï¼Œè¿›è¡Œç»“æ„æ€§è¯„ä¼°'},
            'In': {'severity': 'ä¸­å±', 'suggestion': 'è¯„ä¼°æ‚è´¨å½±å“èŒƒå›´ï¼Œè¿›è¡Œæ— æŸæ£€æµ‹'},
            'Pa': {'severity': 'ä½å±', 'suggestion': 'è¿›è¡Œè¡¨é¢æ¸…ç†å’Œé˜²è…å¤„ç†'},
            'PS': {'severity': 'ä¸­å±', 'suggestion': 'è¯„ä¼°ç‚¹èš€æ·±åº¦ï¼Œè¿›è¡Œè¡¨é¢ä¿®å¤'},
            'RS': {'severity': 'ä½å±', 'suggestion': 'è¿›è¡Œè¡¨é¢æ‰“ç£¨å¤„ç†'},
            'Sc': {'severity': 'ä½å±', 'suggestion': 'è¯„ä¼°åˆ’ç—•æ·±åº¦ï¼Œè¿›è¡Œè¡¨é¢ä¿®å¤'}
        }

        defect_info = defect_descriptions.get(predicted_defect, {
            'severity': 'å¾…è¯„ä¼°', 'suggestion': 'è¯·è¿›è¡Œè¿›ä¸€æ­¥æ£€æµ‹åˆ†æ'
        })

        full_report = f"""
å·¥ä¸šé›¶ä»¶ç¼ºé™·æ£€æµ‹æŠ¥å‘Š - ç”Ÿäº§ç‰ˆæœ¬
================================

æ£€æµ‹åŸºæœ¬ä¿¡æ¯:
------------
â€¢ å›¾åƒæ–‡ä»¶: {os.path.basename(image_path)}
â€¢ æ£€æµ‹æ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}
â€¢ æ£€æµ‹çŠ¶æ€: {review_note}

æ£€æµ‹ç»“æœ:
---------
â€¢ ç¼ºé™·ç±»å‹: {predicted_defect}
â€¢ ç½®ä¿¡åº¦: {confidence:.3f}
â€¢ ä¸¥é‡ç¨‹åº¦: {defect_info['severity']}
â€¢ æ¨¡å‹æè¿°: {generated_text}

å¤„ç†å†³ç­–:
---------
{defect_info['suggestion']}

{"âš ï¸ æ³¨æ„ï¼šç½®ä¿¡åº¦è¾ƒä½ï¼Œå»ºè®®äººå·¥å¤æ ¸ç¡®è®¤" if needs_manual_review else "âœ… æ£€æµ‹ç»“æœå¯é ï¼Œå¯æŒ‰å»ºè®®å¤„ç†"}

================================
"""

        return full_report, predicted_defect, confidence, needs_manual_review


def evaluate_model_performance(model, test_loader, processor, device):
    """å…¨é¢è¯„ä¼°æ¨¡å‹æ€§èƒ½"""

    model.eval()
    all_predictions = []
    all_labels = []
    all_confidences = []

    with torch.no_grad():
        for batch in test_loader:
            if batch is None:
                continue

            # è¿‡æ»¤æ— æ•ˆæ ·æœ¬
            valid_samples = []
            for i in range(len(batch['image'])):
                if batch['image'][i] is not None:
                    valid_samples.append(i)

            if len(valid_samples) == 0:
                continue

            images = torch.stack([batch['image'][i] for i in valid_samples]).to(device)

            defect_labels_indices = []
            for i in valid_samples:
                label = batch['defect_type'][i]
                try:
                    idx = ['Cr', 'In', 'Pa', 'PS', 'RS', 'Sc'].index(label)
                    defect_labels_indices.append(idx)
                except ValueError:
                    continue

            if len(defect_labels_indices) == 0:
                continue

            defect_labels = torch.tensor(defect_labels_indices).to(device)

            outputs = model(pixel_values=images)
            defect_logits = outputs['defect_logits']

            confidences, predicted = torch.max(torch.softmax(defect_logits, dim=1), 1)

            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(defect_labels.cpu().numpy())
            all_confidences.extend(confidences.cpu().numpy())

    if len(all_predictions) == 0:
        print("æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ ·æœ¬")
        return {}, np.array([])

    # è®¡ç®—åˆ†ç±»æŠ¥å‘Š
    target_names = ['Cr', 'In', 'Pa', 'PS', 'RS', 'Sc']
    report = classification_report(all_labels, all_predictions,
                                   target_names=target_names, output_dict=True)

    # è®¡ç®—æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(all_labels, all_predictions)

    # è¯¯æŠ¥åˆ†æ
    false_positive_rate = np.sum((np.array(all_predictions) != np.array(all_labels)) &
                                 (np.array(all_confidences) > 0.8)) / len(all_predictions)

    print("=== æ¨¡å‹æ€§èƒ½è¯„ä¼°æŠ¥å‘Š ===")
    print(f"æ•´ä½“å‡†ç¡®ç‡: {report['accuracy']:.4f}")
    print(f"è¯¯æŠ¥ç‡: {false_positive_rate:.4f}")
    print("\nå„ç±»åˆ«æ€§èƒ½:")
    for class_name in target_names:
        if class_name in report:
            class_report = report[class_name]
            print(f"{class_name}: ç²¾ç¡®ç‡={class_report['precision']:.3f}, "
                  f"å¬å›ç‡={class_report['recall']:.3f}, F1={class_report['f1-score']:.3f}")

    return report, cm


# ä¸»æ‰§è¡Œå‡½æ•° - ä¿®å¤ç‰ˆæœ¬
if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f'ä½¿ç”¨è®¾å¤‡: {device}')

    # æ•°æ®é›†è·¯å¾„
    data_path = "./neu-surface-defect-database"

    if not os.path.exists(data_path):
        print(f"é”™è¯¯ï¼šæ•°æ®é›†è·¯å¾„ {data_path} ä¸å­˜åœ¨ï¼")
        exit(1)

    print(f"æ•°æ®é›†è·¯å¾„: {data_path}")

    # æ£€æŸ¥æ•°æ®é›†ç»“æ„
    defect_types = ['Cr', 'In', 'Pa', 'PS', 'RS', 'Sc']
    found_images = 0
    for defect in defect_types:
        defect_path = os.path.join(data_path, defect)
        if os.path.exists(defect_path):
            image_files = [f for f in os.listdir(defect_path) if f.lower().endswith(('.bmp', '.png', '.jpg', '.jpeg'))]
            found_images += len(image_files)
            print(f"æ‰¾åˆ°ç¼ºé™·ç±»åˆ« {defect}: {len(image_files)} å¼ å›¾åƒ")

    if found_images == 0:
        print("é”™è¯¯ï¼šæœªæ‰¾åˆ°ä»»ä½•å›¾åƒæ–‡ä»¶ï¼")
        exit(1)

    # å‡†å¤‡æ•°æ®
    print("\nå‡†å¤‡æ•°æ®...")
    train_paths, val_paths, test_paths, train_labels, val_labels, test_labels = prepare_data(data_path)

    print(f"è®­ç»ƒé›†æ ·æœ¬æ•°: {len(train_paths)}")
    print(f"éªŒè¯é›†æ ·æœ¬æ•°: {len(val_paths)}")
    print(f"æµ‹è¯•é›†æ ·æœ¬æ•°: {len(test_paths)}")

    # åˆ›å»ºå·¥ä¸šçº§æ•°æ®å˜æ¢
    train_transform, eval_transform = create_industrial_transforms()

    # åˆ›å»ºå¢å¼ºçš„æ•°æ®é›†
    train_dataset = RobustSurfaceDefectDataset(train_paths, train_labels, train_transform, is_train=True)
    val_dataset = RobustSurfaceDefectDataset(val_paths, val_labels, eval_transform, is_train=False)
    test_dataset = RobustSurfaceDefectDataset(test_paths, test_labels, eval_transform, is_train=False)

    # ä½¿ç”¨drop_last=Trueç¡®ä¿æ‰¹æ¬¡å¤§å°ä¸€è‡´
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=2, drop_last=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=2, drop_last=True)
    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=2, drop_last=False)

    # åˆå§‹åŒ–ç”Ÿäº§å°±ç»ªæ¨¡å‹
    print("\nåˆå§‹åŒ–ç”Ÿäº§å°±ç»ªæ¨¡å‹...")
    model_path = "./models/blip-image-captioning-base"

    if not os.path.exists(model_path):
        print(f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹è·¯å¾„ {model_path}")
        exit(1)

    print(f"ä»æœ¬åœ°è·¯å¾„åŠ è½½æ¨¡å‹: {model_path}")
    processor = BlipProcessor.from_pretrained(model_path)
    model = ProductionReadyDefectModel(text_model_name=model_path).to(device)

    # è®­ç»ƒæ¨¡å‹
    print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    training_history = train_model_with_monitoring(
        model, train_loader, val_loader, processor, device, num_epochs=5
    )

    # åŠ è½½æœ€ä½³æ¨¡å‹
    if os.path.exists('best_defect_detection_model.pth'):
        model.load_state_dict(torch.load('best_defect_detection_model.pth'))
        print("å·²åŠ è½½æœ€ä½³æ¨¡å‹")
    else:
        print("ä½¿ç”¨æœ€ç»ˆè®­ç»ƒçš„æ¨¡å‹")

    # å…¨é¢æ€§èƒ½è¯„ä¼°
    print("\nè¿›è¡Œæ¨¡å‹æ€§èƒ½è¯„ä¼°...")
    performance_report, confusion_mat = evaluate_model_performance(model, test_loader, processor, device)

    # ç”Ÿæˆç”Ÿäº§çº§æµ‹è¯•æŠ¥å‘Š
    print("\nç”Ÿæˆç”Ÿäº§çº§æµ‹è¯•æŠ¥å‘Š...")
    if len(test_paths) > 0:
        for i in range(min(5, len(test_paths))):
            test_image = test_paths[i]
            true_label = test_labels[i]

            print(f"\nå¤„ç†å›¾åƒ {i + 1}/{min(5, len(test_paths))}: {os.path.basename(test_image)}")
            print(f"çœŸå®æ ‡ç­¾: {true_label}")

            try:
                report, predicted_defect, confidence, needs_review = generate_production_report(
                    model, test_image, processor, device, confidence_threshold=0.8
                )

                print(report)

                # ä¿å­˜ç”Ÿäº§çº§æŠ¥å‘Š
                status = "REVIEW" if needs_review else "AUTO"
                report_filename = f"production_report_{i + 1}_{predicted_defect}_{status}.txt"
                with open(report_filename, 'w', encoding='utf-8') as f:
                    f.write(report)
                print(f"ç”Ÿäº§æŠ¥å‘Šå·²ä¿å­˜åˆ° {report_filename}")

                # éªŒè¯ç»“æœ
                if predicted_defect == true_label:
                    status_icon = "ğŸŸ¢" if not needs_review else "ğŸŸ¡"
                    print(f"{status_icon} é¢„æµ‹æ­£ç¡®! ç½®ä¿¡åº¦: {confidence:.3f}")
                else:
                    print(f"ğŸ”´ é¢„æµ‹é”™è¯¯! é¢„æµ‹: {predicted_defect}, çœŸå®: {true_label}")
            except Exception as e:
                print(f"ç”ŸæˆæŠ¥å‘Šæ—¶å‡ºé”™: {str(e)}")
                continue

    print("\nğŸ¯ ç”Ÿäº§å°±ç»ªç³»ç»Ÿéƒ¨ç½²å®Œæˆï¼")
    if training_history:
        print("ğŸ“Š å…³é”®æŒ‡æ ‡:")
        print(f"   - æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {max([h['val_accuracy'] for h in training_history]):.4f}")
    if performance_report:
        print(f"   - æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {performance_report['accuracy']:.4f}")
    print("ğŸ“ è¾“å‡ºæ–‡ä»¶:")
    print("   - best_defect_detection_model.pth (æœ€ä½³æ¨¡å‹)")
    print("   - production_report_*.txt (ç”Ÿäº§æ£€æµ‹æŠ¥å‘Š)")
    print("ğŸš€ ç³»ç»Ÿå·²å‡†å¤‡å¥½éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒï¼")